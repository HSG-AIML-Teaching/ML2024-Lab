{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" style=\"max-width: 200px; height: auto\" src=\"./hsg_logo.png\">\n",
    "\n",
    "##  Lab 08 - Autoencoder Neural Networks\n",
    "\n",
    "Machine Learning, University of St. Gallen, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analyses of the **Machine Learning** lecture at the **University of St.Gallen's Artificial Intelligence and Machine Learning [AI:ML]** Group are based on Jupyter Notebooks. These notebooks enable various data analyses and statistical validations to be carried out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 700px\" src=\"./lab_08_banner.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will apply the **deep autoencoder neural networks** (AENNs) introduced in the lecture to detect anomalies in the accounting records of financial accounting. Unlike classical feedforward networks, AENNs learn to **encode** input data into a low-dimensional representation. At the same time, the AENN learns to **decode** the original data from the encoded representation.\n",
    "\n",
    "The decoded data, commonly referred to as the **reconstruction**, should be very similar to the original **input data**. Therefore, accounting entries that can only be reconstructed with errors must exhibit one or more unusual characteristics. The following figure provides an overview of the deep learning process and the AENN network architecture, which we will implement in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 900px\" src=\"./process.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the lab, we will again use some functions of the `PyTorch` library to implement and train the AENN. Throughout the training process, the AENN should learn the characteristic features of historical **bookings** or **journal entries**. After successful model training, we will apply the model to detect unusual journal entries within the dataset based on the reconstruction error. Finally, we will use the learned **representations** of the individual journal entries to interpret the obtained results even more meaningfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of any questions, please do not hesitate to contact us via **marco (dot) schreyer (at) unisg (dot) ch**. We wish you a lot of fun with our notebooks and your audit analyses!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives of the Lab:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After today's exercise, you should be able to:\n",
    "\n",
    ">1. Understand the **basic concepts, functionality, and components** of autoencoder neural networks.\n",
    ">2. Perform **preprocessing** of categorical financial data (i.e., one-hot encoding and min-max normalization).\n",
    ">3. Apply autoencoder neural networks to detect **anomalies** in extensive financial data.\n",
    ">4. Interpret the results or **reconstruction error** of autoencoder neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the Analysis Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous exercises, we will first import a series of Python libraries that enable data analysis and visualization. In this exercise, we will use the libraries `PyTorch`, `Pandas`, `Numpy`, `Scikit-Learn`, `Matplotlib`, and `Seaborn`. Below, we import the required libraries by executing the following instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python data science and utility libraries\n",
    "import os, sys, itertools, urllib, io, warnings\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import pandas_datareader as dr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the `PyTorch` deep learning libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torch.utils.data import dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the `Matplotlib` and `Seaborn` visualization libraries and set the visualization parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "plt.rcParams['figure.dpi']= 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn off possible warnings, e.g., due to future changes in the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the warning filter flag to ignore warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate the so-called inline display of visualizations in Jupyter Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create subdirectories within the current working directory for (1) saving the original data, (2) the analysis results, and (3) the trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the data sub-directory\n",
    "data_directory = './01_data'\n",
    "if not os.path.exists(data_directory): os.makedirs(data_directory)\n",
    "    \n",
    "# create the results sub-directory\n",
    "results_directory = './02_results'\n",
    "if not os.path.exists(results_directory): os.makedirs(results_directory)\n",
    "\n",
    "# create the models sub-directory\n",
    "models_directory = './03_models'\n",
    "if not os.path.exists(models_directory): os.makedirs(models_directory) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting a random seed to ensure reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init deterministic seed\n",
    "seed_value = 1234\n",
    "np.random.seed(seed_value) # set numpy seed\n",
    "torch.manual_seed(seed_value); # set pytorch seed cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activating `GPU` computing by setting the device flag and setting a random `CUDA` seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cpu or gpu enabled device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu').type\n",
    "\n",
    "# set pytorch gpu seed\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "\n",
    "# log type of device enabled\n",
    "now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "print('[LOG {}] notebook with {} computation enabled'.format(str(now), str(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying hardware information for the available GPU(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying software information for the available `Python` and `PyTorch` versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print current Python version\n",
    "now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "print('[LOG {}] The Python version: {}'.format(now, sys.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print current PyTorch version\n",
    "now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "print('[LOG {}] The PyTorch version: {}'.format(now, torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Acquisition and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays, companies are accelerating the digitization of business processes, which also impacts Enterprise Resource Planning (ERP) systems. These systems gather large amounts of granular data, particularly an organization's journal entries, which are recorded within the general ledger and respective subsidiary ledgers.\n",
    "\n",
    "**Figure 1** presents a hierarchical view of an ERP system that captures journal entries in database tables. In the context of audit reviews, the data captured in these systems can contain traces or valuable clues pointing to potential fraudulent actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"max-width: 600px; height: auto\" src=\"./accounting.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 1:** Hierarchical view of an Enterprise Resource Planning (ERP) system capturing business transactions at various abstraction levels in database tables, i.e., at level (1) of the business process, (2) of accounting, and (3) of the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, we will descriptively analyze the dataset used in the lab. Then, we will preprocess the data to create a baseline for training a neural network. The lab dataset is based on a modified subset of the **\"Synthetic Financial Dataset For Fraud Detection\"** dataset by Lopez-Rojas. The original dataset was initially published on the Kaggle platform for data science competitions and can be accessed via the following link: https://www.kaggle.com/ntnu-testimon/paysim1.\n",
    "\n",
    "First, we load the dataset into our analysis environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset into the notebook\n",
    "url = 'https://raw.githubusercontent.com/HSG-AIML-Teaching/ML2024-Lab/main/lab_8/01_data/fraud_dataset_small.csv'\n",
    "ori_dataset = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we verify the dimensionality of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the datasets dimensionalities\n",
    "now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "print('[LOG {}] transactional dataset of {} rows and {} columns retreived.'.format(now, ori_dataset.shape[0], ori_dataset.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we save a backup copy of the loaded dataset with the current timestamp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine current timestamp \n",
    "timestamp = dt.datetime.utcnow().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# define dataset filename \n",
    "filename = timestamp + \" - original_fraud_dataset.xlsx\"\n",
    "\n",
    "# save dataset extract to the data directory\n",
    "ori_dataset.head(100).to_excel(os.path.join(data_directory, filename)) # just saving the first 100 rows as a sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Initial Data Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains a total of **seven categorical** and **two numerical attributes** corresponding to the tables BKPF (accounting document headers) and BSEG (accounting document segments) within an SAP FICO module. The list below provides an overview of the individual attributes, along with a brief description of their respective semantics:\n",
    "\n",
    ">- `BELNR`: the accounting document number,\n",
    ">- `BUKRS`: the company code,\n",
    ">- `BSCHL`: the posting key,\n",
    ">- `HKONT`: the posted general ledger account,\n",
    ">- `PRCTR`: the posted profit center,\n",
    ">- `WAERS`: the currency key,\n",
    ">- `KTOSL`: the key of the general ledger account,\n",
    ">- `DMBTR`: the amount in the local currency,\n",
    ">- `WRBTR`: the amount in the document currency.\n",
    "\n",
    "Let's also examine the first 10 rows of the dataset in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect top rows of dataset\n",
    "ori_dataset.head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed the attribute labeled `Label` in the data while reviewing it. This attribute contains the ground truth information for each individual journal entry. The attribute describes the 'true nature' of each transaction, i.e., whether it is a **regular** transaction (marked by `regular`) or a **anomaly** (marked by `global` and `local`).\n",
    "\n",
    "In our approach, we will use the label information solely to validate the results of our trained models. However, please note that such a field often isn't available in real-life scenarios. Now let's examine the distribution of regular transactions versus anomalous transactions in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of anomalies vs. regular transactions\n",
    "ori_dataset.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis reveals that, similar to the real world, we are dealing with an **unbalanced** dataset. The dataset contains only a tiny proportion of **100 (0.109%)** anomalous transactions in total. Among the 100 anomalies, there are **70 (0.076%)** `global` anomalies and **30 (0.003%)** `local` anomalies.\n",
    "\n",
    "In the next step, we remove the label attribute from the training dataset and store it in a separate variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the \"ground-truth\" label information for the following steps of the class\n",
    "label = ori_dataset.pop('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preprocessing of Categorical Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon examining the data, it's evident that the majority of attributes have categorical (discrete) attribute values, such as the journal entry date, the main ledger account, the journal entry type, and the currency. Let's take a closer look at the distribution of the categorical attributes *posting key* `BSCHL` and *general ledger account* `HKONT`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to plot posting key and general ledger account side by side\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "fig.set_figwidth(18)\n",
    "\n",
    "# plot the distribution of the posting key attribute\n",
    "plot = sns.countplot(x=ori_dataset['BSCHL'], ax=ax[0])\n",
    "\n",
    "# set axis labels\n",
    "plot.set_xticklabels(plot.get_xticklabels(), rotation=90)\n",
    "plot.set_xlabel('Attribute Value', fontsize=16)\n",
    "plot.set_ylabel('Attribute Value Count', fontsize=16)\n",
    "\n",
    "# set plot title\n",
    "plot.set_title('Posting Key - Attribute Value Distribution', fontsize=16)\n",
    "\n",
    "# plot the distribution of the general ledger attribute\n",
    "plot = sns.countplot(x=ori_dataset['HKONT'], ax=ax[1])\n",
    "\n",
    "# set axis labels\n",
    "plot.set_xticklabels(plot.get_xticklabels(), rotation=90)\n",
    "plot.set_xlabel('Attribute Value', fontsize=16)\n",
    "plot.set_ylabel('Attribute Value Count', fontsize=16)\n",
    "\n",
    "# set plot title\n",
    "plot.set_title('General Ledger - Attribute Value Distribution', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, neural networks are designed to process numerical data. One way to meet this requirement is to apply a method known as **One-Hot Encoding**. This allows a numerical representation of categorical attribute values to be derived. With **One-Hot Encoding**, an additional binary column is created in the data for each categorical attribute value.\n",
    "\n",
    "Consider the example in **Figure 2** below. The categorical attribute Receiver in the original data contains the names 'Sally', 'John', and 'Emma'. We encode the attribute as a 'one-hot' attribute by creating an additional binary column for each categorical value in the 'Receiver' column. Then, for example, we encode each transaction with the value 'Sally' in the 'Receiver' column with the value 1.0 within the 'Sally' column of the transaction. If a transaction has a different value in the 'Receiver' column, we encode the 'Sally' column with the value 0.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"max-width: 500px; height: auto\" src=\"./encoding.png\">\n",
    "\n",
    "**Figure 2:** Exemplary **One-Hot Encoding** of the different receiver attribute values into specific binary one-hot columns. Each observed attribute value in the dataset results in its own column. The column value **1.0** encodes the occurrence of the attribute value in the corresponding journal entry. The column value **0.0**, on the other hand, indicates that the attribute value does not occur within the corresponding journal entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this method, the six categorical attributes of the dataset can be converted into numerical attributes. The `Pandas` library offers the appropriate functionality, which we apply in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select categorical attributes to be \"one-hot\" encoded\n",
    "categorical_attr_names = ['BUKRS', 'KTOSL', 'PRCTR', 'BSCHL', 'HKONT', 'WAERS']\n",
    "\n",
    "# encode categorical attributes into a binary one-hot encoded representation \n",
    "ori_dataset_cat_processed = pd.get_dummies(ori_dataset[categorical_attr_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterward, we verify the performed **One-Hot Encoding** using the first 10 journal entries of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect encoded sample transactions\n",
    "ori_dataset_cat_processed.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Preprocessing of Numerical Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we analyze the distributions of the two numerical attributes of the dataset. These are the attributes (1) *Amount in local currency* `DMBTR` and (2) *Amount in document currency* `WRBTR`, the distributions of which we will visualize below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the log-scaled 'DMBTR' as well as the 'WRBTR' attribute value distribution\n",
    "fig, ax = plt.subplots(1,2)\n",
    "fig.set_figwidth(18)\n",
    "\n",
    "# plot distribution of the local amount attribute\n",
    "plot = sns.distplot(ori_dataset['DMBTR'].tolist(), ax=ax[0])\n",
    "\n",
    "# set axis labels\n",
    "plot.set_xlabel('Attribute Value', fontsize=16)\n",
    "plot.set_ylabel('Attribute Value Count', fontsize=16)\n",
    "\n",
    "# set plot title\n",
    "plot.set_title('Amount in Local Currency - Attribute Value Distribution', fontsize=16)\n",
    "\n",
    "# plot distribution of the document amount attribute\n",
    "plot = sns.distplot(ori_dataset['WRBTR'].tolist(), ax=ax[1])\n",
    "\n",
    "# set axis labels\n",
    "plot.set_xlabel('Attribute Value', fontsize=16)\n",
    "plot.set_ylabel('Attribute Value Count', fontsize=16)\n",
    "\n",
    "# set plot title\n",
    "plot.set_title('Amount in Document Currency - Attribute Value Distribution', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of both amount attributes exhibit a **skewed** and **steep distribution**. Therefore, we will first scale the values logarithmically. Subsequently, we will min-max normalize the scaled values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the 'DMBTR' and 'WRBTR' attribute\n",
    "numeric_attr_names = ['DMBTR', 'WRBTR']\n",
    "\n",
    "# add a small epsilon to eliminate zero values from data for log scaling\n",
    "numeric_attr = ori_dataset[numeric_attr_names] + 1e-7\n",
    "\n",
    "# log scale the 'DMBTR' and 'WRBTR' attribute values\n",
    "numeric_attr = numeric_attr.apply(np.log)\n",
    "\n",
    "# normalize all numeric attributes to the range [0,1]\n",
    "ori_dataset_num_processed = (numeric_attr - numeric_attr.min()) / (numeric_attr.max() - numeric_attr.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we visualize the distributions of the scaled and normalized values for both amount attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the log-scaled 'DMBTR' as well as the 'WRBTR' attribute value distribution\n",
    "fig, ax = plt.subplots(1,2)\n",
    "fig.set_figwidth(18)\n",
    "\n",
    "# plot distribution of the local amount attribute\n",
    "plot = sns.distplot(ori_dataset_num_processed['DMBTR'].tolist(), ax=ax[0])\n",
    "\n",
    "# set axis labels\n",
    "plot.set_xlabel('Attribute Value', fontsize=16)\n",
    "plot.set_ylabel('Attribute Value Count', fontsize=16)\n",
    "\n",
    "# set plot title\n",
    "plot.set_title('Betrag in Hauswährung - Attribute Value Distribution', fontsize=16)\n",
    "\n",
    "# plot distribution of the document amount attribute\n",
    "plot = sns.distplot(ori_dataset_num_processed['WRBTR'].tolist(), ax=ax[1])\n",
    "\n",
    "# set axis labels\n",
    "plot.set_xlabel('Attribute Value', fontsize=16)\n",
    "plot.set_ylabel('Attribute Value Count', fontsize=16)\n",
    "\n",
    "# set plot title\n",
    "plot.set_title('Betrag in Dokumentwährung - Attribute Value Distribution', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Merging Categorical and Numerical Transaction Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we merge the preprocessed numerical and categorical attributes into a **single dataset**. The merged dataset will serve as the foundation for the subsequent training of the Autoencoder Neural Network (AENN):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge categorical and numeric subsets\n",
    "ori_subset_transformed = pd.concat([ori_dataset_cat_processed, ori_dataset_num_processed], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a final look at the dimensionality of the merged dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect final dimensions of pre-processed transactional data\n",
    "ori_subset_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the preprocessing steps, we have a dataset consisting of a total of **91,147 records (rows)** and **618 attributes (columns)**. We should keep the number of columns in mind, as it will determine the dimensionality of the input and output layers of our AENN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Autoencoder Neural Network Implementierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Abschnitt möchten wir uns mit der zugrundeliegenden Idee und dem Aufbau eines tiefen **Autoencoder Neural Networks (AENN)** vertraut zu machen. Hierzu werden wir die einzelne Bausteine und die spezifische Netzwerkstruktur von AENNs anhand der `PyTorch` Open-Source-Bibliothek implementieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Autoencoder Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Autoencoder Neural Networks*, also referred to as *Replicator Neural Networks*, are an unsupervised learning variant of classic feed-forward networks. This particular architecture was originally developed by Geoffrey Hinton and Ruslan Salakhutdinov. AENNs typically consist of a **symmetrical** network architecture and a central hidden layer, referred to as the **latent** or **bottleneck** layer. This layer has a lower dimensionality than the input and output layers of the network. The learning objective of the AENN is to reconstruct the original input data as accurately as possible at the output layer of the network. **Figure 3** shows a schematic representation of an Autoencoder Neural Network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"max-width: 800px; height: auto\" src=\"./autoencoder.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 3:** Schematic representation of an **Autoencoder Neural Network**, which consists of two non-linear mappings or feed-forward networks. The two interconnected networks are referred to as the **Encoder** $f_\\theta: \\mathbb{R}^{dx} \\mapsto \\mathbb{R}^{dz}$ and **Decoder** $g_\\theta: \\mathbb{R}^{dz} \\mapsto \\mathbb{R}^{dx}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, AENNs can be interpreted as 'lossy' **compression algorithms**. They are 'lossy' in the sense that the reconstructed outputs may have errors compared to the original inputs. The difference between the original input $x^i$ and its **reconstruction** $\\hat{x}^i$ is also referred to as the reconstruction error. In general, AENNs consist of three main components:\n",
    "\n",
    "> 1. an Encoder $f_\\theta$,\n",
    "> 2. a Decoder $g_\\theta$,\n",
    "> 3. an error function $\\mathcal{L_{\\theta}}$.\n",
    "\n",
    "The encoder and decoder each consist of a classic feedforward network with learnable parameters $\\theta$. The **encoder** $f_\\theta(\\cdot)$ maps an input vector (e.g., a journal entry) $x^i$ onto a compressed (i.e., low-dimensional) representation $z^i$ in the so-called latent space $Z$. The low-dimensional representation $z^i$ is then mapped by the **decoder** $g_\\theta(\\cdot)$ onto an output vector $\\hat{x}^i$ of the original input space (e.g., the reconstructed journal entry) is mapped. Formally, the two networks can also be interpreted as non-linear mappings or functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>$f_\\theta(x^i) = s(Wx^i + b)$ &emsp; $f_\\theta: \\mathbb{R}^{dx} \\mapsto \\mathbb{R}^{dz}$,</center>\n",
    "<center>$g_\\theta(z^i) = s′(W′z^i + d)$ &emsp; $g_\\theta: \\mathbb{R}^{dz} \\mapsto \\mathbb{R}^{dx}$,</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the two functions have the learnable model parameters $\\theta = {W, b, W', d}$. The parameters $W \\in \\mathbb{R}^{d_x \\times d_z}, W' \\in \\mathbb{R}^{d_z \\times d_y}$ denote the weight matrices, the parameters $b \\in \\mathbb{R}^{dx}$, $d \\in \\mathbb{R}^{dz}$ the bias vectors of the networks, and $s$ and $s′$ the respective non-linear activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Autoencoder Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we want to implement the Encoder Network in `PyTorch`. The encoder should consist of a total of **nine layers** of fully-connected neurons. In addition, the encoder should contain the following number of neurons per layer: 618-256-128-64-32-16-8-4-3. The previous notation means that the first layer comprises 618 neurons (determined by the dimensionality of the input data), the second layer 256 neurons, and the other layers 128, 64, 32, 16, 8, 4, and 3 neurons, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"max-width: 900px; height: auto\" src=\"./neurons.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following three elements of the encoder implementation deserve special attention:\n",
    "\n",
    ">- `self.encoder_Lx`: defines the linear transformation of the respective layer applied to the input: $Wx + b$.\n",
    ">- `nn.init.xavier_uniform`: initializes weight parameters based on a uniform Xavier distribution.\n",
    ">- `self.encoder_Rx`: defines the non-linear transformation of the respective layer applied to the input: $\\sigma(\\cdot)$.\n",
    "\n",
    "We use so-called **Leaky ReLUs** to avoid saturating neurons and to accelerate training convergence. The application of Leaky ReLUs allows the calculation of gradients even within the negative range of an activation function (see figure above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of the encoder network\n",
    "class encoder(nn.Module):\n",
    "\n",
    "    # define class constructor\n",
    "    def __init__(self):\n",
    "\n",
    "        # call super class constructor\n",
    "        super(encoder, self).__init__()\n",
    "\n",
    "        # specify layer 1 - in 618, out 512\n",
    "        self.encoder_L1 = nn.Linear(in_features=ori_subset_transformed.shape[1], out_features=512, bias=True) # add linearity \n",
    "        nn.init.xavier_uniform_(self.encoder_L1.weight) # init weights\n",
    "        self.encoder_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity \n",
    "\n",
    "        # specify layer 2 - in 512, out 256\n",
    "        self.encoder_L2 = nn.Linear(512, 256, bias=True)\n",
    "        nn.init.xavier_uniform_(self.encoder_L2.weight)\n",
    "        self.encoder_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 3 - in 256, out 128\n",
    "        self.encoder_L3 = nn.Linear(256, 128, bias=True)\n",
    "        nn.init.xavier_uniform_(self.encoder_L3.weight)\n",
    "        self.encoder_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 4 - in 128, out 64\n",
    "        self.encoder_L4 = nn.Linear(128, 64, bias=True)\n",
    "        nn.init.xavier_uniform_(self.encoder_L4.weight)\n",
    "        self.encoder_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 5 - in 64, out 32\n",
    "        self.encoder_L5 = nn.Linear(64, 32, bias=True)\n",
    "        nn.init.xavier_uniform_(self.encoder_L5.weight)\n",
    "        self.encoder_R5 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 6 - in 32, out 16\n",
    "        self.encoder_L6 = nn.Linear(32, 16, bias=True)\n",
    "        nn.init.xavier_uniform_(self.encoder_L6.weight)\n",
    "        self.encoder_R6 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 7 - in 16, out 8\n",
    "        self.encoder_L7 = nn.Linear(16, 8, bias=True)\n",
    "        nn.init.xavier_uniform_(self.encoder_L7.weight)\n",
    "        self.encoder_R7 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 8 - in 8, out 4\n",
    "        self.encoder_L8 = nn.Linear(8, 4, bias=True)\n",
    "        nn.init.xavier_uniform_(self.encoder_L8.weight)\n",
    "        self.encoder_R8 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 9 - in 4, out 3\n",
    "        self.encoder_L9 = nn.Linear(4, 3, bias=True)\n",
    "        nn.init.xavier_uniform_(self.encoder_L9.weight)\n",
    "        self.encoder_R9 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "    # define forward pass\n",
    "    def forward(self, x):\n",
    "\n",
    "        # define forward pass through the network\n",
    "        x = self.encoder_R1(self.encoder_L1(x))\n",
    "        x = self.encoder_R2(self.encoder_L2(x))\n",
    "        x = self.encoder_R3(self.encoder_L3(x))\n",
    "        x = self.encoder_R4(self.encoder_L4(x))\n",
    "        x = self.encoder_R5(self.encoder_L5(x))\n",
    "        x = self.encoder_R6(self.encoder_L6(x))\n",
    "        x = self.encoder_R7(self.encoder_L7(x))\n",
    "        x = self.encoder_R8(self.encoder_L8(x))\n",
    "        x = self.encoder_R9(self.encoder_L9(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate a model of the encoder network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intstantiate the encoder network model\n",
    "encoder_train = encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, we transfer the Encoder model to the `CPU` or a potentially available `GPU`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push model to compute device\n",
    "encoder_train = encoder_train.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If available, we check whether the model has been successfully transferred to the `GPU`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize the model structure and review the network architecture once more by executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the initialized architectures\n",
    "now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "print('[LOG {}] encoder architecture:\\n\\n{}\\n'.format(now, encoder_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will complete the autoencoder architecture by implementing the corresponding decoder network. The decoder should also consist of a total of **nine layers** of fully-connected neurons. Additionally, the decoder should **symmetrically mirror** the architecture of the encoder. To accomplish this, we will invert the design of the encoder's layers layer-by-layer, following the structure 3-4-8-16-32-64-128-256, during the implementation of the decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of the decoder network\n",
    "class decoder(nn.Module):\n",
    "\n",
    "    # define class constructor\n",
    "    def __init__(self):\n",
    "\n",
    "        # call super class constructor\n",
    "        super(decoder, self).__init__()\n",
    "\n",
    "        # specify layer 1 - in 3, out 4\n",
    "        self.decoder_L1 = nn.Linear(in_features=3, out_features=4, bias=True) # add linearity \n",
    "        nn.init.xavier_uniform_(self.decoder_L1.weight)  # init weights\n",
    "        self.decoder_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity\n",
    "\n",
    "        # specify layer 2 - in 4, out 8\n",
    "        self.decoder_L2 = nn.Linear(4, 8, bias=True)\n",
    "        nn.init.xavier_uniform_(self.decoder_L2.weight)\n",
    "        self.decoder_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 3 - in 8, out 16\n",
    "        self.decoder_L3 = nn.Linear(8, 16, bias=True)\n",
    "        nn.init.xavier_uniform_(self.decoder_L3.weight)\n",
    "        self.decoder_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 4 - in 16, out 32\n",
    "        self.decoder_L4 = nn.Linear(16, 32, bias=True)\n",
    "        nn.init.xavier_uniform_(self.decoder_L4.weight)\n",
    "        self.decoder_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 5 - in 32, out 64\n",
    "        self.decoder_L5 = nn.Linear(32, 64, bias=True)\n",
    "        nn.init.xavier_uniform_(self.decoder_L5.weight)\n",
    "        self.decoder_R5 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 6 - in 64, out 128\n",
    "        self.decoder_L6 = nn.Linear(64, 128, bias=True)\n",
    "        nn.init.xavier_uniform_(self.decoder_L6.weight)\n",
    "        self.decoder_R6 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "        \n",
    "        # specify layer 7 - in 128, out 256\n",
    "        self.decoder_L7 = nn.Linear(128, 256, bias=True)\n",
    "        nn.init.xavier_uniform_(self.decoder_L7.weight)\n",
    "        self.decoder_R7 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 8 - in 256, out 512\n",
    "        self.decoder_L8 = nn.Linear(256, 512, bias=True)\n",
    "        nn.init.xavier_uniform_(self.decoder_L8.weight)\n",
    "        self.decoder_R8 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 9 - in 512, out 618\n",
    "        self.decoder_L9 = nn.Linear(in_features=512, out_features=ori_subset_transformed.shape[1], bias=True)\n",
    "        nn.init.xavier_uniform_(self.decoder_L9.weight)\n",
    "        self.decoder_R9 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "    # define forward pass\n",
    "    def forward(self, x):\n",
    "\n",
    "        # define forward pass through the network\n",
    "        x = self.decoder_R1(self.decoder_L1(x))\n",
    "        x = self.decoder_R2(self.decoder_L2(x))\n",
    "        x = self.decoder_R3(self.decoder_L3(x))\n",
    "        x = self.decoder_R4(self.decoder_L4(x))\n",
    "        x = self.decoder_R5(self.decoder_L5(x))\n",
    "        x = self.decoder_R6(self.decoder_L6(x))\n",
    "        x = self.decoder_R7(self.decoder_L7(x))\n",
    "        x = self.decoder_R8(self.decoder_L8(x))\n",
    "        x = self.decoder_R9(self.decoder_L9(x)) # don't apply dropout to the AE output\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will instantiate the decoder model for `CPU` or `GPU` training and ensure that the model has been successfully initialized. To do this, we will visualize the network architecture by executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intstantiate the decoder network model\n",
    "decoder_train = decoder()\n",
    "\n",
    "# push model to compute device\n",
    "decoder_train = decoder_train.to(device)\n",
    "    \n",
    "# print the initialized architectures\n",
    "now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "print('[LOG {}] decoder architecture:\\n\\n{}\\n'.format(now, decoder_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's take a look at the number of model parameters we intend to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the number of encoder model parameters\n",
    "encoder_num_params = 0\n",
    "\n",
    "# iterate over the distinct encoder parameters\n",
    "for param in encoder_train.parameters():\n",
    "\n",
    "    # collect number of parameters\n",
    "    encoder_num_params += param.numel()\n",
    "\n",
    "# init the number of decoder model parameters\n",
    "decoder_num_params = 0\n",
    "    \n",
    "# iterate over the distinct decoder parameters\n",
    "for param in decoder_train.parameters():\n",
    "\n",
    "    # collect number of parameters\n",
    "    decoder_num_params += param.numel()\n",
    "    \n",
    "# print the number of model paramters\n",
    "now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "print('[LOG {}] number of to be trained AENN model parameters: {}.'.format(now, encoder_num_params + decoder_num_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, our AENN model comprises a considerable total of **985,021** model parameters to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Autoencoder Neural Network Parameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successfully instantiating the AENN model, we now want to train the model. However, before we start training, it is necessary to define a suitable error function. As a reminder: We want to train our model in such a way that it learns a set of encoder and decoder parameters $θ$ that maximizes the similarity between a given journal entry $x^{i}$ and its reconstruction $\\hat{x}^{i} = g_θ(f_θ(x^{i}))$.\n",
    "\n",
    "Formally expressed, our training objective is to learn parameters $θ^*$ such that $\\arg\\min_{\\theta} |X - g_\\theta(f_\\theta(X))|$. To achieve this optimization goal, we want to continuously minimize the error function or a so-called **reconstruction error** $\\mathcal{L_{\\theta}}$ as training progresses. A suitable error function for this purpose is the so-called **Binary Cross-Entropy (BCE)** reconstruction error, which is formally defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> $\\mathcal{L^{BCE}_{\\theta}}(x^{i};\\hat{x}^{i}) = \\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{k} x^{i}_{j} ln(\\hat{x}^{i}_{j}) + (1-x^{i}_{j}) ln(1-\\hat{x}^{i}_{j})$, </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $x^{i}$, $i=1,...,n$ denotes the set of journal entries, $\\hat{x}^{i}$ the respective reconstructions, and $j=1,...,k$ indexes the various journal entry attributes. In the following, we instantiate the corresponding BCE error function from the `PyTorch` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimization criterion / loss function\n",
    "loss_function = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we transfer the computation of the error function to the `CPU` or an available `GPU` if applicable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push the optimization criterion / loss function to compute device \n",
    "loss_function = loss_function.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the error magnitude of a mini-batch of journal entries, the `PyTorch` library automatically calculates the gradients. Subsequently, the AENN parameters $θ$ are optimized based on the determined gradients. To achieve this, it is only necessary to define the desired optimization method in `PyTorch`. In the following notebook cell, we use the so-called **Adam Optimization** method for optimizing the model parameters $θ$. Additionally, we define a learning rate $l = 0.0001$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the learning rate\n",
    "learning_rate = 1e-4\n",
    "\n",
    "#set the paramete optimization strategy of both networks\n",
    "encoder_optimizer = torch.optim.Adam(encoder_train.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.Adam(decoder_train.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successfully implementing and instantiating the three building blocks of the AENN model, let's take the time to review the definitions of the **Encoder** and **Decoder** models as well as the **BCE Reconstruction Error** and discuss any questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Autoencoder Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we aim to train an AENN model using the encoded transaction data. Moreover, we will take a detailed look at the individual training hyperparameters, training steps, and the training progress over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Definition der Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by training an AENN model for **5 training epochs** and **128 journal entries** per mini-batch. This configuration of hyperparameters means that the dataset is fed to the AENN a total of five times in mini-batches of 128 bookings each. As a result, each training epoch will have **713 updates** (91,247 bookings modulo 128 bookings per mini-batch) of the AENN model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify training parameters\n",
    "num_epochs = 5 # number of training epochs\n",
    "mini_batch_size = 128 # size of the mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training phase, the AENN model should continuously receive mini-batches of the entire population of journal entries. For this purpose, we use the `DataLoader` functionality of the`PyTorch` library. These are iterators that continuously provide the bookings in the form of mini-batches. In the following cell, we instantiate a `PyTorch DataLoader` of the journal entry data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pre-processed transactional data to PyTorch tensor\n",
    "torch_dataset = torch.from_numpy(ori_subset_transformed.values).float()\n",
    "\n",
    "# push pre-processed transactional data to compute device\n",
    "torch_dataset = torch_dataset.to(device)\n",
    "\n",
    "# init training dataloader\n",
    "train_dataloader = dataloader.DataLoader(torch_dataset, batch_size=mini_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: By setting the parameter `shuffle`, the distinct mini-batches contain random journal entries of the datasets provided in a different order for each epoch.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training des Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the hyperparameters, we can start training the model. For each supplied mini-batch, the following steps are performed during the training process:\n",
    "\n",
    ">1. Perform the forward pass through the encoder and decoder network.\n",
    ">2. Calculate the BCE reconstruction error $\\mathcal{L^{BCE}_{\\theta}}(x^{i};\\hat{x}^{i})$.\n",
    ">3. Perform the backward pass through the decoder and encoder network.\n",
    ">4. Update the encoder $f_\\theta(\\cdot)$ and decoder $g_\\theta(\\cdot)$ parameters.\n",
    "\n",
    "To ensure learning during training, we monitor the BCE reconstruction error of the AENN model as the training progresses. By observing this, it is possible to infer the model's learning progress. Additionally, we can determine if and when the reconstruction error converges.\n",
    "\n",
    "During the model optimization, we want to pay special attention to the following `PyTorch` instructions:\n",
    "\n",
    ">- `reconstruction_loss.backward()`: Calculates the gradients based on the reconstruction error.\n",
    ">- `encoder_optimizer.step()` and `decoder_optimizer.step()`: Update the parameters based on the gradients `encoder_optimizer.step()` and `decoder_optimizer.step()`: Update the parameters based on the gradients`.\n",
    "\n",
    "After each completed training epoch, we also want to save a so-called **model checkpoint**. Checkpoints contain a snapshot of the model parameters at that point in time. In general, it is a good practice to save such checkpoints at regular intervals during training. If the training is interrupted, it can be resumed from the last checkpoint. To save a model checkpoint, we use the following `PyTorch` instruction:\n",
    "\n",
    ">- `torch.save()`: Saves the checkpoint of the current model parameter values to the local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init collection of training epoch losses\n",
    "train_epoch_losses = []\n",
    "\n",
    "# set the model in training mode (apply dropout when needed)\n",
    "encoder_train.train()\n",
    "decoder_train.train()\n",
    "\n",
    "# init the best loss by setting it to infinity\n",
    "best_loss = np.inf\n",
    "\n",
    "# train autoencoder model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # init collection of epoch losses\n",
    "    train_mini_batch_losses = []\n",
    "    \n",
    "    # init mini batch counter\n",
    "    mini_batch_count = 0\n",
    "        \n",
    "    # iterate over all mini-batches\n",
    "    for mini_batch_data in train_dataloader:\n",
    "\n",
    "        # increase mini batch counter\n",
    "        mini_batch_count += 1\n",
    "\n",
    "        # push mini batch data to compute device\n",
    "        mini_batch_data = mini_batch_data.to(device)\n",
    "\n",
    "        # =================== (1) forward pass ===================================\n",
    "\n",
    "        # run forward pass\n",
    "        z_representation = encoder_train(mini_batch_data) # encode mini-batch data\n",
    "        mini_batch_reconstruction = decoder_train(z_representation) # decode mini-batch data\n",
    "        \n",
    "        # =================== (2) compute reconstruction loss ====================\n",
    "\n",
    "        # determine reconstruction loss\n",
    "        reconstruction_loss = loss_function(mini_batch_reconstruction, mini_batch_data)\n",
    "        \n",
    "        # =================== (3) backward pass ==================================\n",
    "\n",
    "        # reset graph gradients\n",
    "        decoder_optimizer.zero_grad()\n",
    "        encoder_optimizer.zero_grad()\n",
    "\n",
    "        # run backward pass\n",
    "        reconstruction_loss.backward()\n",
    "        \n",
    "        # =================== (4) update model parameters ========================\n",
    "\n",
    "        # update network parameters\n",
    "        decoder_optimizer.step()\n",
    "        encoder_optimizer.step()\n",
    "\n",
    "        # =================== monitor training progress ==========================\n",
    "\n",
    "        # print training progress each 1.000 mini-batches\n",
    "        if mini_batch_count % 1000 == 0:\n",
    "            \n",
    "            # print mini batch reconstuction results\n",
    "            now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "            print('[LOG {}] epoch: [{}/{}], batch: {}, batch-train-loss: {}'.format(str(now), str(epoch+1), str(num_epochs), str(mini_batch_count), str(np.round(reconstruction_loss.item(), 8))))\n",
    "            \n",
    "        # collect mini-batch loss\n",
    "        train_mini_batch_losses.extend([reconstruction_loss.item()])\n",
    "\n",
    "    # =================== evaluate model performance =============================\n",
    "    \n",
    "    # determine mean min-batch loss of epoch\n",
    "    train_epoch_loss = np.mean(train_mini_batch_losses)\n",
    "                                 \n",
    "    # print training epoch results\n",
    "    now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "    print('[LOG {}] epoch: [{}/{}], epoch-train-loss: {}'.format(str(now), str(epoch+1), str(num_epochs), str(np.round(train_epoch_loss, 8))))\n",
    "\n",
    "    # determine mean min-batch loss of epoch\n",
    "    train_epoch_losses.append(train_epoch_loss)\n",
    "    \n",
    "    # =================== save model snapshot to disk ============================\n",
    "    \n",
    "    # case: new best model trained\n",
    "    if train_epoch_loss < best_loss:\n",
    "    \n",
    "        # save trained encoder model file to disk\n",
    "        encoder_model_name = \"ep_{}_encoder_model.pth\".format((epoch+1))\n",
    "        torch.save(encoder_train.state_dict(), os.path.join(models_directory, encoder_model_name))\n",
    "\n",
    "        # save trained decoder model file to disk\n",
    "        decoder_model_name = \"ep_{}_decoder_model.pth\".format((epoch+1))\n",
    "        torch.save(decoder_train.state_dict(), os.path.join(models_directory, decoder_model_name))\n",
    "        \n",
    "        # update best loss\n",
    "        best_loss = train_epoch_loss\n",
    "\n",
    "        # print epoch loss\n",
    "        now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "        print('[LOG {}] epoch: [{}/{}], new best epoch-train-loss: {} found'.format(str(now), str(epoch+1), str(num_epochs), str(np.round(train_epoch_loss, 8))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will visualize the reconstruction error for each training epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "fig.set_figwidth(18)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot the training epochs vs. the epochs' prediction error\n",
    "ax.plot(np.array(range(1, len(train_epoch_losses)+1)), train_epoch_losses, label='epoch loss (blue)')\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel(\"[Training Epoch $e_i$]\", fontsize=14)\n",
    "ax.set_ylabel(\"[Reconstruction Error $\\mathcal{L}^{BCE}$]\", fontsize=14)\n",
    "\n",
    "# set plot legend\n",
    "plt.legend(loc=\"upper right\", numpoints=1, fancybox=True)\n",
    "\n",
    "# add plot title\n",
    "plt.title('Training Epochs $e_i$ vs. Reconstruction Error $L^{BCE}$', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that the reconstruction error of the AENN model begins to decrease continuously after five epochs. This observation implies that the model is gradually succeeding in reconstructing the journal entries contained within the dataset. However, the visualization also shows that the model could be trained for several more epochs until the reconstruction error no longer decreases or converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Autoencoder Neural Network Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we want to evaluate the learned AENN model's ability to detect anomalies in journal entry data. We will use pre-trained AENN models for this purpose. The evaluation includes the **local** as well as the **global** anomalies of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Loading a Model Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the evaluation, we typically load the AENN model with the **lowest** reconstruction error or use another, i.e., already pre-trained, model. During training, we saved a checkpoint of the model parameters for each epoch within the local model directory. We will now load a model checkpoint that has already been trained for **30 training epochs**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore pretrained model checkpoint\n",
    "encoder_model_name = 'https://raw.githubusercontent.com/HSG-AIML-Teaching/ML2024-Lab/main/lab_8/03_models/ep_30_encoder_model_small.pth'\n",
    "decoder_model_name = 'https://raw.githubusercontent.com/HSG-AIML-Teaching/ML2024-Lab/main/lab_8/03_models/ep_30_decoder_model_small.pth'\n",
    "\n",
    "# read stored model from the remote location\n",
    "encoder_bytes = urllib.request.urlopen(encoder_model_name)\n",
    "decoder_bytes = urllib.request.urlopen(decoder_model_name)\n",
    "\n",
    "# load tensor from io.BytesIO object\n",
    "encoder_buffer = io.BytesIO(encoder_bytes.read())\n",
    "decoder_buffer = io.BytesIO(decoder_bytes.read())\n",
    "\n",
    "# init evaluation encoder and decoder model\n",
    "encoder_eval = encoder()\n",
    "decoder_eval = decoder()\n",
    "\n",
    "# push encoder and decoder model to compute device\n",
    "encoder_eval = encoder_train.to(device)\n",
    "decoder_eval = decoder_train.to(device)\n",
    "\n",
    "# load trained models\n",
    "encoder_eval.load_state_dict(torch.load(encoder_buffer, map_location=lambda storage, loc: storage))\n",
    "decoder_eval.load_state_dict(torch.load(decoder_buffer, map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Evaluation des Modells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successfully loading the model checkpoint, we transfer the model for evaluation purposes to the `CPU` (Note: This allows us to calculate the reconstruction errors of all journal entries without any limitations due to the `GPU` memory):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set networks in evaluation mode (don't apply dropout)\n",
    "encoder_eval.eval()\n",
    "decoder_eval.eval()\n",
    "\n",
    "# push encoder and decoder model to compute device\n",
    "encoder_eval = encoder_eval.to('cpu')\n",
    "decoder_eval = decoder_eval.to('cpu')\n",
    "\n",
    "# push the dataset to the CPU \n",
    "torch_dataset = torch_dataset.to('cpu')\n",
    "\n",
    "# push the loss function to the CPU\n",
    "loss_function = loss_function.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we calculate the **individual** BCE reconstruction errors for each journal entry $x_{i}$ within the dataset. First, the reconstruction $\\hat{x}{i}$ of each entry is determined. Second, the BCE reconstruction error of the reconstructed entry $\\hat{x}{i}$ is calculated by comparing it to the original journal entries $x_{i}$ from the dataset:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct encoded transactional data\n",
    "reconstruction = decoder_eval(encoder_eval(torch_dataset))\n",
    "\n",
    "# init binary cross entropy errors\n",
    "reconstruction_loss_transaction = np.zeros(reconstruction.size()[0])\n",
    "\n",
    "# iterate over all detailed reconstructions\n",
    "for i in range(0, reconstruction.size()[0]):\n",
    "\n",
    "    # determine reconstruction loss - individual transactions\n",
    "    reconstruction_loss_transaction[i] = loss_function(reconstruction[i], torch_dataset[i]).item()\n",
    "\n",
    "    if(i % 10000 == 0):\n",
    "\n",
    "        ### print conversion summary\n",
    "        now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "        print('[LOG {}] collected individual reconstruction loss of: {:06}/{:06} transactions'.format(now, i, reconstruction.size()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating the individual reconstruction errors, we visualize the magnitude of each error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# set plot size\n",
    "fig.set_figwidth(16)\n",
    "fig.set_figheight(8)\n",
    "\n",
    "# assign unique id to transactions\n",
    "plot_data = np.column_stack((np.arange(len(reconstruction_loss_transaction)), reconstruction_loss_transaction))\n",
    "\n",
    "# obtain regular transactions as well as global and local anomalies\n",
    "regular_data = plot_data[label == 'regular']\n",
    "global_outliers = plot_data[label == 'global']\n",
    "local_outliers = plot_data[label == 'local']\n",
    "\n",
    "# plot reconstruction error scatter plot\n",
    "ax.scatter(regular_data[:, 0], regular_data[:, 1], c='C0', alpha=0.4, marker=\"o\", s=30, label='regular') # plot regular transactions\n",
    "ax.scatter(global_outliers[:, 0], global_outliers[:, 1], c='C1', marker=\"^\", s=80, label='global') # plot global outliers\n",
    "ax.scatter(local_outliers[:, 0], local_outliers[:, 1], c='C2', marker=\"*\", s=80, label='local') # plot local outliers\n",
    "\n",
    "# add plot legend of transaction classes\n",
    "ax.legend(loc='best')\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel(\"[Journal Entry ID $x_i$]\", fontsize=14)\n",
    "ax.set_ylabel(\"[Reconstruction Error $\\mathcal{L}^{BCE}$]\", fontsize=14)\n",
    "\n",
    "# add plot title\n",
    "plt.title('Journal Entry ID $x_i$ vs. Reconstruction Error $L^{BCE}$', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization shows that our AENN model can reconstruct most regular journal entries with low error. Simultaneously, both the **global anomalies** (green) and **local anomalies** (red) exhibit a comparatively high reconstruction error. Based on this analysis result, we can conclude that it is possible to differentiate **anomalies** (green and red) from regular journal entries (blue) within the dataset using reconstruction errors\n",
    "\n",
    "To further investigate this observation, we will now filter out journal entries that have a **reconstruction error >= 0.12** from the dataset. Additionally, we assume (as demonstrated above) that these journal entries correspond to the **global anomalies** within the total population of journal entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append labels to original dataset\n",
    "ori_dataset['label'] = label\n",
    "\n",
    "# extract transactions exhibiting a reconstruction error >= 0.12\n",
    "autoencoder_global_anomalies = ori_dataset[reconstruction_loss_transaction >= 0.12]\n",
    "\n",
    "# inspect transactions exhibiting a reconstruction error >= 0.12\n",
    "autoencoder_global_anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now extract the journal entries into an Excel table to make them available to the audit team. First, we will generate a timestamp of the data extract for the audit trail of the audit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = dt.datetime.utcnow().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we extract the filtered **global anomalies** as an Excel file for further substantive testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the filename of the excel spreadsheet\n",
    "filename = str(timestamp) + \" - ACA_001_autoencoder_global_anomalies.xlsx\"\n",
    "\n",
    "# specify the target data directory of the excel spreadsheet\n",
    "data_directory = os.path.join(results_directory, filename)\n",
    "\n",
    "# extract the filtered transactions to excel\n",
    "autoencoder_global_anomalies.to_excel(data_directory, header=True, index=False, sheet_name=\"Global_Anomalies\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's closely examine the journal entries with a **reconstruction error >= 0.04 but <= 0.12**. We assume (as illustrated above) that these journal entries correspond to the **local anomalies** within the population of journal entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract transactions exhibiting a reconstruction error < 0.12 and >= 0.04\n",
    "autoencoder_local_anomalies = ori_dataset[(reconstruction_loss_transaction >= 0.04) & (reconstruction_loss_transaction < 0.12)]\n",
    "\n",
    "# inspect transactions exhibiting a reconstruction error < 0.12 and >= 0.04\n",
    "autoencoder_local_anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll again extract the filtered journal entries into an Excel table to make them available to the audit team. First, we will generate a timestamp of the data extract for the audit trail of the audit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = dt.datetime.utcnow().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following that, we'll extract the filtered **local anomalies** as an Excel file for further substantive testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the filename of the excel spreadsheet\n",
    "filename = str(timestamp) + \" - ACA_002_autoencoder_local_anomalies.xlsx\"\n",
    "\n",
    "# specify the target data directory of the excel spreadsheet\n",
    "data_directory = os.path.join(results_directory, filename)\n",
    "\n",
    "# extract the filtered transactions to excel\n",
    "autoencoder_local_anomalies.to_excel(data_directory, header=True, index=False, sheet_name=\"Local_Anomalies\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation of Journal Entry Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a real-world audit context, it's generally beneficial to examine the **learned representations** of the journal entries by the AENN model in addition to the reconstruction error. This examination allows for insights into the **structural semantics** of the entries and corresponding attributes. Furthermore, the analysis offers the possibility to contextualize any identified anomalies within the population of journal entries.\n",
    "\n",
    "To obtain the representation of the journal entry, we perform a forward pass through the encoder of the AENN model for each journal entry. To do this, we load an encoder network **model checkpoint** trained for 80 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore pretrained model checkpoint\n",
    "encoder_model_name = 'https://raw.githubusercontent.com/HSG-AIML-Teaching/ML2024-Lab/main/lab_8/03_models/ep_80_encoder_model_small.pth'\n",
    "\n",
    "# read stored model from the remote location\n",
    "encoder_bytes = urllib.request.urlopen(encoder_model_name)\n",
    "\n",
    "# load tensor from io.BytesIO object\n",
    "encoder_buffer = io.BytesIO(encoder_bytes.read())\n",
    "\n",
    "# init evaluation encoder and decoder model\n",
    "encoder_eval = encoder()\n",
    "\n",
    "# push encoder and decoder model to compute device\n",
    "encoder_eval = encoder_eval.to('cpu')\n",
    "\n",
    "# load trained models\n",
    "encoder_eval.load_state_dict(torch.load(encoder_buffer, map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we perform a forward pass through the AENN model for each journal entry. Consequently, we obtain the three-dimensional representation of each entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push the dataset to the CPU \n",
    "torch_dataset = torch_dataset.to('cpu')\n",
    "\n",
    "# run forward path through encoder to obtain journal entry representations\n",
    "entry_representations = encoder_eval(torch_dataset)\n",
    "\n",
    "# convert the representations to a pandas dataframe\n",
    "entry_representation = pd.DataFrame(entry_representations.data.cpu().numpy(), columns=['z1', 'z2', 'z3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, for validation and visualization purposes, we label the representations with the original labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_representation['label'] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before visualizing, let's carefully examine the obtained coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_representation.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the three-dimensional **latent space**, we can visualize the space and the representations using the `Matplotlib 3D` functionality. In the following cell, we create a visualization of this space and the representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enforce inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# import 3d plotting and animation libraries\n",
    "from IPython.display import HTML\n",
    "from matplotlib import animation\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "# init the plot\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# change plot perspective\n",
    "ax.view_init(elev=30, azim=240)\n",
    "\n",
    "# set axis paramaters of subplot\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot regular transactions, just the first 1000 to gain an intuition\n",
    "regular = entry_representation[entry_representation['label'] == 'regular']\n",
    "ax.scatter(regular['z1'][0:2000], regular['z2'][0:2000], regular['z3'][0:2000], c='C0', alpha=0.4, marker=\"o\", label='regular')\n",
    "\n",
    "# plot first order anomalous transactions\n",
    "global_anomalies = entry_representation[entry_representation['label'] == 'global']\n",
    "ax.scatter(global_anomalies['z1'], global_anomalies['z2'], global_anomalies['z3'], c='C1', s=100, marker=\"^\", label='global')\n",
    "\n",
    "# plot second order anomalous transactions\n",
    "local_anomalies = entry_representation[entry_representation['label'] == 'local']\n",
    "ax.scatter(local_anomalies['z1'], local_anomalies['z2'], local_anomalies['z3'], c='C2', s=100, marker=\"*\", label='local')\n",
    "\n",
    "# set axis labels\n",
    "ax.set_xlabel('activation [$z_1$]', weight='normal', fontsize=12)\n",
    "ax.set_ylabel('activation [$z_2$]', weight='normal', fontsize=12)\n",
    "ax.set_zlabel('activation [$z_3$]', weight='normal', fontsize=12)\n",
    "\n",
    "# add plot legend of transaction classes\n",
    "ax.legend(loc='best')\n",
    "\n",
    "# set plot title\n",
    "plt.title('AEEN Model Latent Space', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Aufgaben:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deepen your understanding, we recommend working through the following exercises:\n",
    "\n",
    "**1. Train and evaluate an Autoencoder Neural Network model with a reduced bottleneck.**\n",
    "\n",
    "> The architecture presented within the notebook led to a good model for anomaly detection. Check how the performance changes when the dimensionality of the bottleneck layer is reduced. To do this, reduce the number of neurons within the encoder and decoder bottlenecks to two neurons. How does the model's ability to detect anomalies in the journal entry data change? Can different statements be made for global and local anomalies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# You can insert you solution here. \n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Train and evaluate a shallow Autoencoder Neural Network model.**\n",
    "\n",
    "> The architecture presented within the notebook led to a good model for anomaly detection. Check how the performance changes when several hidden layers are removed. To do this, adjust the implementations of the encoder and decoder accordingly. How does the model's ability to detect anomalies in the journal entry data change? Can different statements be made for global and local anomalies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# You can insert you solution here.\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provided a step-by-step introduction to the **design, implementation, training, and evaluation** of a neural network-based approach to anomaly detection in accounting data. The code examples and exercises presented can serve as a starting point for the development and testing of more complex strategies for anomaly detection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
